---
title: "Statistics II"
author: "
<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <font style='font-style:normal'>Introduction to R</font><br>
      <a href='https://dwulff.github.io/Intro2R_Unibe/'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:therbootcamp@gmail.com'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <font style='font-style:normal'>Bern R Bootcamp</font>
      </a>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr>
</table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, warning=F, message=F}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)
```

<p align="center">
<img width="100%" src="image/hansrosling.png" margin=0><br>
<font style="font-size:10px">from [TED.com](https://www.ted.com/talks/hans_rosling_the_truth_about_hiv)</font>
</p>

# {.tabset}


## Overview

In this practical you'll do basic statistics in R. By the end of this practical you will know how to:

1. Calculate regression analyses with `glm()` and `lm()`.
2. Explore statistical objects with `names()`, `summary()`, `print()`, `predict()`.
3. Run mixed-model and Bayesian regressions using the `lme4` and `BayesFactor` packages.


## Tasks

```{r, echo = FALSE, eval = TRUE, include = FALSE}

library(broom)
library(rsq)
library(lubridate)
library(tidyverse)

gap <- read_csv("1_Data/gap.csv")

```


### A - Getting setup

1. Open your `BernRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that all of the data files listed in the `Datasets` section are contained in the `1_Data` folder.

```{r}
# Done!
```

2. Open a new R script and save it as a new file called `statisticsII_practical.R` in the `2_Code` folder. At the top of the script, using comments, write your name and the date. The, load the set of packages listed in the `Functions` section with `library()`.

3. For this practical, we'll use the `gap.csv` data. This dataset contains data from three years on economic and health variables of 139 countries. Using the following template, load the data into R and store it as a new object called `kc_house`.

```{r, echo = TRUE, eval = FALSE}
gap <- read_csv(file = "XX")
```

```{r}
gap <- read_csv(file = "1_Data/gap.csv")
```

4. Using `print()`, `summary()`, and `head()`, explore the data to make sure it was loaded correctly.

```{r}
gap
summary(gap)
head(gap)
```

### B - Regression models with `lm()`

1. To begin with, evaluate the relationship between the health expenditure of the country (`healthExp`) and child mortality (`childMort`) using a regression with child mortality as the criterion and health expenditure as the predictor. Simply complete the code below.

```{r, eval = FALSE, echo = TRUE}
mort_lm <- lm(formula = XX ~ XX,
              data = XX)
```


```{r}
mort_lm <- lm(formula = childMort ~ healthExp,
              data = gap)
```

2. Print your `mort_lm` object to see the main results.

```{r}
mort_lm
```


3. Use the `summary()` function to return a data frame containing the main results of the linear regression. Evalute the test results: Does health expenditure significantly predict child mortality? How much variance is explained by health expenditure (Tipp: look for Multiple R-squared)?

```{r}
summary(mort_lm)
```

4. You probably concluded correctly that health expenditure correctly predicts child mortality. One way to confirm this is via a simple plot by plugging the two variables into the code below (don't worry you learn how to do fancier plots in the next sessions).

```{r, echo = TRUE, eval = FALSE}
plot(gap$XX, gap$XX, log = "xy")
```

```{r}
plot(gap$healthExp, gap$childMort, log = "xy")
```

5. Health expenditure is strongly related to child mortality, but is it the best predictor. Run the regression again, this time adding the GDP per capita (`gdpPercap`) as a second predictor.

```{r, eval = FALSE, echo = TRUE}
mort_lm <- lm(formula = XX ~ XX + XX,
              data = XX)
```

```{r}
mort_lm <- lm(formula = childMort ~ healthExp + gdpPercap,
              data = gap)
```

6. Have the results changed? Which predictor is more strongly related to child mortality? Evaluate using `summary()`.  

```{r}
summary(mort_lm)
```

7. GDP per capita is the better predictor of child mortality. Now play around a bit. Find out if the two predictors interact or whether any of the other `numeric` (!) varibles significantly improve the prediction of child mortality.

### C - ANOVA models with `lm()`

1. ANOVA's in R are also run using `lm()`. Technically, an ANOVA is a regression using dummy variables to code each of the predictor levels in the ANOVA. To see this, predict child mortality using `continent` and print the resulting model.

```{r, eval = FALSE, echo = TRUE}
mort_lm <- lm(formula = XX ~ continent,
              data = XX)
mort_lm
```

```{r}
mort_lm <- lm(formula = childMort ~ continent,
              data = gap)
mort_lm
```

2. You should have observed that 


4. Evaluate the p-value of this test compare to what you got in the previous question when you used `cor.test()`?

```{r}
# Same!
```


5. Which of the variables `bedrooms`, `bathrooms`, and `sqft_living` predict housing price? Answer this by conducting the appropriate regression analysis using `lm()` and assigning the result to an object `price_lm`. Use the following template:

```{r, eval = FALSE, echo = TRUE}
price_lm <- lm(formula = XX ~ XX + XX + XX,
               data = XX)
```

```{r}
price_lm <- lm(formula = price ~ bedrooms + bathrooms + sqft_living,
               data = kc_house)
```


6. Print your `price_lm` object to see the main results.

```{r}
price_lm
```

7. Apply the `summary()` function to `price_lm`. Do you see anything new or important?

```{r}
summary(price_lm)
```

8. Look at the names of the `price_lm` with `names()`. What are the named elements of the object?

```{r}
names(price_lm)
```

9. Using the `$` operator, print a vector of the estimated coefficients of the analysis.

```{r}
price_lm$coefficients
```

10. Use the `tidy()` function (from the `broom` package) to return a dataframe containing the main results of the test.

```{r}
tidy(price_lm)
```


11. Use the `rsq()` function (from the `rsq` package) to obtain the R-squared value. Does this match what you saw in your previous outputs?

```{r}
rsq(price_lm)
```

12. Using the following template, create a new model called `everything_lm` that predicts housing prices based on *all* predictors in `kc_house` *except* for `id` and `date` (`id` is meaningless and date shouldn't matter). Use the following template. Note that to include all predictors, we'll use the `formula = y ~.` shortcut. We'll also remove id and date from the dataset using `select()` before running the analysis.

```{r, echo = TRUE, eval = FALSE}
everything_lm <- lm(formula = XX ~.,
                    data = kc_house %>% 
                             select(-id, -date))
```


```{r}
everything_lm <- lm(formula = price ~.,
                    data = kc_house %>% 
                             select(-id, -date))
```

13. Print your `everything_lm` object to see the main results.


```{r}
everything_lm
```

14. Apply the `summary()` function to `everything_lm`. Do you see anything new or important?

```{r}
summary(everything_lm)
```

15. Using the `$` operator, print a vector of the estimated coefficients of the analysis. Are the beta values for bedrooms, bathrooms, and sqft_living different from what you got in your previous analysis `price_lm`?

```{r}
everything_lm$coefficients
```

16. Use the `tidy()` function to return a dataframe containing the main results of the test.

```{r}
tidy(everything_lm)
```

17. Use the `rsq()` function (from the `rsq` package) to obtain the R-squared value. How does this R-squared value compare to what you got in your previous regression `price_lm`?

```{r}
rsq(everything_lm)
```

18. How well did the `everything_lm` model fit the actual housing prices? We can answer this by calculating the average difference between the fitted values and the true values directly. Using the following template, make this calculation. What do you find is the mean absolute difference between the fitted housing prices and the true prices?

```{r, echo = TRUE, eval = FALSE}
# True housing prices
prices_true <- kc_house$XX

# Model fits (fitted.values)
prices_fitted <- everything_lm$XX

# Calculate absolute error between fitted and true values
abs_error <- abs(XX - XX)

# Calculate mean absolute error
mae <- mean(XX)

# Print the result!
mae
```


```{r}
# True housing prices
prices_true <- kc_house$price

# Model fits (fitted.values)
prices_fitted <- everything_lm$fitted.values

# Calculate absolute error between fitted and true values
abs_error <- abs(prices_true - prices_fitted)

# Calculate mean absolute error
mae <- mean(abs_error)

# Print the result!
mae
```


19. Using the following template, create a scatterplot showing the relationship between the fitted values of the `everything_lm` object and the true prices.

```{r, echo = TRUE, eval = FALSE}
# Create dataframe containing fitted and true prices
prices_results <- tibble(truth = XX,
                         fitted = XX)

# Create scatterplot
ggplot(data = prices_results,
       aes(x = fitted, y = truth)) + 
  geom_point(alpha = .1) +
  geom_smooth(method = 'lm') +
  labs(title = "Fitted versus Predicted housing prices",
       subtitle = paste0("Mean Absolute Error = ", mae),
       caption = "Source: King County housing price Kaggle dataset",
       x = "Fitted housing price values",
       y = 'True values') + 
  theme_minimal()
```

```{r}
# Create dataframe containing fitted and true prices
prices_results <- tibble(truth = kc_house$price,
                         fitted = everything_lm$fitted.values)

# Create scatterplot
ggplot(data = prices_results,
       aes(x = fitted, y = truth)) + 
  geom_point(alpha = .1) +
  geom_smooth(method = 'lm') +
  labs(title = "Fitted versus Predicted housing prices",
       subtitle = paste0("Mean Absolute Error = ", mae),
       caption = "Source: King County housing price Kaggle dataset",
       x = "Fitted housing price values",
       y = 'True values') + 
  theme_minimal()
```



### F - Logistic regression with glm(family = 'binomial')

1. In the next set of analyses, we'll use logistic regression to predict whether or not a house will sell for over \$1,000,000. First, we'll need to create a new binary variable in the `kc_house` called `million` that is 1 when the price is over 1 million, and 0 when it is not. Run the following code to create this new variable

```{r, echo = TRUE}
# Create a new binary variable called million that
#  indicates when houses sell for more than 1 million

# Note: 1e6 is a shortcut for 1000000

kc_house <- kc_house %>%
                mutate(million = price > 1e6)
```

2. Using the following template, use the `glm()` function to conduct a logistic regression to see which of the variables `bedrooms`, `bathrooms`, `floors`, `waterfront`, `yr_built` predict whether or not a house will sell for over 1 Million. Be sure to include the argument `family = 'binomial'` to tell `glm()` that we are conducting a logistic regression analysis. 

```{r, echo = TRUE, eval = FALSE}
# Logistic regression analysis predicting which houses will sell for
#   more than 1 Million

million_glm <- glm(formula = XX ~ XX + XX + XX + XX + XX,
                   data = kc_house,
                   family = "XX")
```

```{r}
million_glm <- glm(formula = million ~ bedrooms + bathrooms + floors + waterfront + yr_built,
                   family = "binomial",   # Logistic regression
                   data = kc_house)
```

3. Print your `million_glm` object to see the main results.

```{r}
million_glm
```


4. Apply the `summary()` function to `million_glm`.


```{r}
summary(million_glm)
```

5. Using the `$` operator, print a vector of the estimated beta values (coefficients) of the analysis.


```{r}
million_glm$coefficients
```

6. Use the `tidy()` function to return a dataframe containing the main results of the test.


```{r}
tidy(million_glm)
```

7. You can get the fitted probability predictions that each house will sell for more than 1 Million by accessing the vector `million_glm$fitted.values`. Using this vector, calculate the average probability that houses will sell for more than 1 Million (hint: just take the mean!)

```{r}
mean(million_glm$fitted.values)
```

8. Using the following code, create a data frame showing the relationship between the fitted probability that a house sells for over 1 Million and the true probability: 

```{r, echo = TRUE, eval = FALSE}
# Just run it!

million_fit <- tibble(pred_million = million_glm$fitted.values,
                      true_million = kc_house$million) %>%
                mutate(fitted_cut = cut(pred_million, breaks = seq(0, 1, .1))) %>%
                group_by(fitted_cut) %>%
                summarise(true_prob = mean(true_million))

million_fit
```

```{r}
# Just run it!

million_fit <- tibble(pred_million = million_glm$fitted.values,
                      true_million = kc_house$million) %>%
                mutate(fitted_cut = cut(pred_million, breaks = seq(0, 1, .1))) %>%
                group_by(fitted_cut) %>%
                summarise(true_prob = mean(true_million))

million_fit
```

9. Now plot the results using the following code:

```{r, echo = TRUE, eval = FALSE}
# Just run it!

ggplot(million_fit, 
       aes(x = fitted_cut, y = true_prob, col = as.numeric(fitted_cut))) + 
  geom_point(size = 2) +
  labs(x = "Fitted Probability",
       y = "True Probability",
       title = "Predicting the probability of a 1 Million house",
       subtitle = "Using logistic regression with glm(family = 'binomial')") +
  scale_y_continuous(limits = c(0, 1)) +
  guides(col = FALSE)
```


```{r}
# Just run it!

ggplot(million_fit, 
       aes(x = fitted_cut, y = true_prob, col = as.numeric(fitted_cut))) + 
  geom_point(size = 2) +
  labs(x = "Fitted Probability",
       y = "True Probability",
       title = "Predicting the probability of a 1 Million house",
       subtitle = "Using logistic regression with glm(family = 'binomial')") +
  scale_y_continuous(limits = c(0, 1)) +
  guides(col = FALSE)
```


### G - Using predict() to predict new values

1. Your friend Donald just bought a house whose building records have been lost. She wants to know what year her house was likely built. Help Donald figure out when his house was built by conducting the appropriate regression analysis, and then using the specifics of his house to predict the year that his house was built using the `predict()` function. You know that his house is on the waterfront, has 3 bedrooms, 2 floors and has a condition of 4. The following block of code may help you!

```{r, echo = TRUE, eval = FALSE}
# Create regression model predicting year (yr_built)
year_lm <- lm(formula = XX ~ XX + XX + XX + XX,
              data = XX)

# Define Donald's House
DonaldsHouse <- tibble(waterfront = X,
                       bedrooms = X,
                       floors = X,
                       condition = X)

# Predict the hear of donald's house
predict(object = year_lm, 
        newdata = DonaldsHouse)
```

```{r}
# Create regression model predicting year (yr_built)
year_lm <- lm(formula = yr_built ~ waterfront + bedrooms + floors + condition,
              data = kc_house)

# Define Donald's House
DonaldsHouse <- tibble(waterfront = 1,
                       bedrooms = 3,
                       floors = 2,
                       condition = 4)

# Predict the hear of donald's house
predict(object = year_lm, 
        newdata = DonaldsHouse)
```

### X - Advanced

1. Which variables in the dataset significantly predict the grade of a house? Answer this with a regression analysis.

```{r}
grade_lm <- lm(formula = grade ~ .,
               data = kc_house %>%
                 select(-id, -date))


tidy(grade_lm)
```


2. Is there an interaction between waterfront and year built (`yr_built`) on price? In other words, is the relationship between year built and price the same for homes on the waterfront and homes not on the waterfront? Answer this by conducting the appropriate regression analysis and interpret the results. Note: to include an interaction in a model, use the formula `y ~ X1 * X2` instead of `y ~ X1 + X2`.

```{r}
int_lm <- lm(formula = price ~ yr_built * waterfront,
             data = kc_house)

tidy(int_lm)
```

3. How well can a regression analysis based on houses sold in 2014 predict housing costs in the 2015? To answer this we'll create a regression model based on houses sold in 2014 and then use that model to predict the prices of houses sold in 2015. To start, we'll need to create two new dataframes `kc_house_2014` and `kc_house_2015`. Run the code in the following chunk to do this.

```{r, echo = TRUE, eval = FALSE}
# Just run it!

# Add year_sold to kc_house

kc_house <- kc_house %>%
  mutate(year_sold = year(ymd(date)))

# Create kc_house_2014, 
#    Only houses sold in 2014
kc_house_2014 <- kc_house %>%
  filter(year_sold == 2014)

# Create kc_house_2015, 
#     Only houses sold in 2015
kc_house_2015 <- kc_house %>%
  filter(year_sold == 2015)
```

4. Create a regression model called `price_2014_lm` that models housing prices in 2014 using only the `kc_house_2014` data. Note that you may wish to exclude some variables such as `id`, `date`, `million` and `year_sold`.

```{r}
# on your own!
```


5. Create a regression model called `price_2015_lm` that models housing prices in 2015 using only the `kc_house_2015` data. Include the same exclusion and transformation criterion you used in the previous question.

```{r}
# on your own!
```


6. Explore and compare the two models, do they look very similar? Very different?

```{r}
# on your own!
```

7. Calculate the mean absolute error of each model applied to the dataset in which they were created. That is, what was the mean absolute fitted error of each model?

```{r}
# on your own!
```

8. Using `predict()`, predict the 2015 housing data based on the `price_2014_lm` model.

```{r}
# on your own!
```

9. Calculate the mean absolute error of the `price_2014_lm` model when applied to the 2015 data. How different was the mean error compared to the mean fitting error of the `price_2015_lm` model?

```{r}
# on your own!
```

### Generating random samples from distributions

10. You can easily generate random samples from statistical distributions in R. To see all of them, run `?distributions`. For example, to generate samples from the well known Normal distribution, you can use `rnorm()`. Look at the help menu for `rnorm()` to see its arguments. 

11. Let's explore the `rnorm()` function. Using `rnorm()`, create a new object `samp_100` which is 100 samples from a Normal distribution with mean 10 and standard deviation 5. Print the object to see what the elements look like. What should the mean and standard deviation of this sample? be? Test it by evaluating its mean and standard deviation directly using the appropriate functions. Then, do a one-sample t-test on this sample against the null hypothesis that the true mean is 10. What are the results? Use the following code template to help!

```{r, echo = TRUE, eval = FALSE}
# Generate 100 samples from a Normal distribution with mean = 10 and sd = 5
samp_100 <- rnorm(n = XX, mean = XX, sd = XX)

# Print result
samp_100

# Calcultae sample mean and standard deviation.
mean(XX)
sd(XX)

t.test(x = XX,   # Vector of values
       mu = XX)  # Mean under null hypothesis
```

```{r}
# Generate 100 samples from a Normal distribution with mean = 100 and sd = 5
samp_100 <- rnorm(n = 100, mean = 10, sd = 5)

# Print result
samp_100

# Calcultae sample mean and standard deviation.
mean(samp_100)
sd(samp_100)

t.test(x = samp_100,   # Vector of values
       mu = 10)  # Mean under null hypothesis
```

12. Repeat the previous block of code several times and look at how the output changes. How do the p-values change when you keep drawing random samples?

13. Change the mean under the null hypothesis to 15 instead of 10 and run the code again several times. What happens to the p-values?

```{r}
t.test(x = samp_100,   # Vector of values
       mu = 15)  # Mean under null hypothesis
```

14. Look at the code below. As you can see, I generate 4 variables x1, x2, x3 and noise. I then create a dependent variable y that is a function of these variables. Finally, I conduct a regression analysis. *Before you run the code*, what do you expect the value of the `coefficients` of the regression equation will be?  Test your prediction by running the code and exploring the `my_lm` object.

```{r, echo = TRUE, eval = FALSE}
# Generate independent variables
x1 <- rnorm(n = 100, mean = 10, sd = 1)
x2 <- rnorm(n = 100, mean = 20, sd = 10)
x3 <- rnorm(n = 100, mean = -5, sd = 5)

# Generate noise
noise <- rnorm(n = 100, mean = 0, sd = 1)

# Create dependent variable
y <- 3 * x1 + 2 * x2 - 5 * x3 + 100 + noise

# Combine all into a tibble
my_data <- tibble(x1, x2, x3, y)

# Calculate my_lm
my_lm <- lm(formula = y ~ x1 + x2 + x3,
              data = my_data)
```

15. Adjust the code above so that the coefficients for the regression equation will be (close to) `(Intercept) = -50`, `x1 = -3`, `x2 = 10`, `x3 = 15`

```{r}
# Generate independent variables
x1 <- rnorm(n = 100, mean = 10, sd = 1)
x2 <- rnorm(n = 100, mean = 20, sd = 10)
x3 <- rnorm(n = 100, mean = -5, sd = 5)

# Generate noise
noise <- rnorm(n = 100, mean = 0, sd = 1)

# Create dependent variable
y <- -3 * x1 + 10 * x2 + 15 * x3 + -50 + noise

# Combine all into a tibble
my_data <- tibble(x1, x2, x3, y)

# Calculate my_lm
my_lm <- lm(formula = y ~ x1 + x2 + x3,
              data = my_data)

my_lm
```

## Examples

```{r, eval = FALSE, echo = TRUE, message = FALSE, warning = FALSE}

# Examples of regression models on the diamonds dataset -------------

library(tidyverse)
library(broom)
library(BayesFactor)
library(lme4)

# First few rows of the diamonds data

diamonds

# Regression ----------------------------

# Q: Run regression predicting price by carat and depth

price_lm <- lm(formula = price ~ carat + depth,
               data = diamonds)

# Print coefficients
price_lm$coefficients

# Regular summary
summary(price_lm)

# Tidy version
tidy(price_lm)


# Q: Run regression predicting price by carat and depth and their interaction

price_lm <- lm(formula = price ~ carat * depth,
               data = diamonds)

# Print coefficients
price_lm$coefficients

# Regular summary
summary(price_lm)

# Tidy version
tidy(price_lm)


# ANOVA ----------------------------

# Q: Run ANOVA predicting price by color and cut

# transform to factor
dimaonds$cut = as.factor(dimaonds$cut)
dimaonds$color = as.factor(dimaonds$color)

# Run model
price_lm <- lm(formula = price ~ cut + color,
               data = diamonds)

# Print coefficients
price_lm$coefficients

# Regular summary
summary(price_lm)

# Tidy version
tidy(price_lm)

# ANOVA version
anova(price_lm)

# Mixed-effects regression ----------------------------

# Q: Run mixed-effect regression predicting carat and depth with the impact of carat varying across color.

# Run model
price_lme <- lmer(formula = price ~ carat + depth + (1 + carat|color),
                  data = diamonds)

# Regular summary
summary(price_lme)


# Bayesian regression ----------------------------

# Q: Run Bayesian regression predicting carat and depth

# Run model
price_Blm <- regressionBF(formula = price ~ carat + depth,
                  data = diamonds)

# Regular summary
price_Blm


```

## Datasets

|File | Rows | Columns | Description |
|:----|:-----|:------|:-----------------------------------------|
|[gap.csv](https://raw.githubusercontent.com/therbootcamp/Intro2R_Unibe/master/_sessions/_StatisticsII/1_Data/gap.csv) | 417 | 9 | Country statistics from 1997, 2002, and 2007 from the Gapminder Foundation. |

The Gapminder Foundation is a non-profit venture registered in Stockholm, Sweden, that promotes sustainable global development and achievement of the United Nations Millennium Development Goals by increased use and understanding of statistics and other information about social, economic and environmental development at local, national and global levels (see [Gapminder.org](https://www.gapminder.org)). The present dataset is an excerpt of their database holding data from three years on economic and health variables of 139 countries.


|File | Rows |
|:-------------|:-------------------------------------|
|country| country name|
|continent| country's continent|
|year| measurement year|
|lifeExp| life expectancy at birth, in years |
|pop| country's population|
|gdpPercap| country's GDP per capita (US$)|
|childMort| child mortality (0-5 year-olds dying per 1,000 born)|
|co2Emit| CO2 emissions (tonnes per person) |
|healthExp| Government health spending per person (international $)|

## Functions

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`lme4`|`install.packages("lme4")`|
|`BayesFactor`|`install.packages("BayesFactor")`|

### Functions

*Model functions*

| Function| Package | Description |
|:------|:-------------------|:-------------------|
|     ``|    | |
|     ``|    | |
|     `glm()`, `lm()`|    Generalized linear model and linear model| |

*Inspect models*

| Function| Package | Description |
|:------|:-------------------|:-------------------|
|     `summary()`|  `base`  | |
|     `anova()`|  `base`  | |
|     `predict()` | `stats` | Predict  |
|     `names()` | `base` | Print names of model contents. |


## Resources

### Documentations

- For more advanced mixed level ANOVAs with random effects, consult the `afex` and `lmer` packages.

- To do Bayesian versions of common hypothesis tests, try using the `BayesFactor` package. [BayesFactor Guide Link](https://cran.r-project.org/web/packages/BayesFactor/vignettes/manual.html)
